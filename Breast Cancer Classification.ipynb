{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Breast Cancer Classification.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Q8X8xDwS6vgS"},"source":["#**Breast Cancer Classification using Logistic Regression**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PEGG4I6n60cM"},"source":["## Library Import"]},{"cell_type":"code","metadata":{"id":"nE75VGB6-B70","executionInfo":{"status":"ok","timestamp":1624215322085,"user_tz":-330,"elapsed":723,"user":{"displayName":"Akash Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitYhMLI_o2TmDi-sZDOgdYyI8RJFvVdIV0-pHwIg=s64","userId":"08911264043000151771"}}},"source":["import pandas as pd\n","import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fI535KRU-J8k"},"source":["> At first we have imported the required libraries like pandas, numpy etc."]},{"cell_type":"markdown","metadata":{"id":"BbToI8jI_Ts-"},"source":["## Dataset Import"]},{"cell_type":"code","metadata":{"id":"kh9Et71o_boe","executionInfo":{"status":"ok","timestamp":1624215322564,"user_tz":-330,"elapsed":37,"user":{"displayName":"Akash Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitYhMLI_o2TmDi-sZDOgdYyI8RJFvVdIV0-pHwIg=s64","userId":"08911264043000151771"}}},"source":["data = pd.read_csv('breast-cancer-wisconsin.csv')"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z1U0z-IpDn46"},"source":["> Dataset has been imported using pandas read_csv method. Now we will check the information of the dataset.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ldjHr2JAtXd","executionInfo":{"status":"ok","timestamp":1624215322567,"user_tz":-330,"elapsed":38,"user":{"displayName":"Akash Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitYhMLI_o2TmDi-sZDOgdYyI8RJFvVdIV0-pHwIg=s64","userId":"08911264043000151771"}},"outputId":"1306fcda-de1a-4a70-e07c-92b5c43ab15f"},"source":["data.info()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 699 entries, 0 to 698\n","Data columns (total 11 columns):\n"," #   Column                       Non-Null Count  Dtype \n","---  ------                       --------------  ----- \n"," 0   Sample code number           699 non-null    int64 \n"," 1   Clump Thickness              699 non-null    int64 \n"," 2   Uniformity of Cell Size      699 non-null    int64 \n"," 3   Uniformity of Cell Shape     699 non-null    int64 \n"," 4   Marginal Adhesion            699 non-null    int64 \n"," 5   Single Epithelial Cell Size  699 non-null    int64 \n"," 6   Bare Nuclei                  699 non-null    object\n"," 7   Bland Chromatin              699 non-null    int64 \n"," 8   Normal Nucleoli              699 non-null    int64 \n"," 9   Mitoses                      699 non-null    int64 \n"," 10  Class                        699 non-null    int64 \n","dtypes: int64(10), object(1)\n","memory usage: 60.2+ KB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GBcPb8EQEFS9"},"source":["> Here we can see 'Bare Nuclei' is of object type which is undesirable. So let's check for which values it become object type instead on integer type."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":858},"id":"o2iiTnHKEpnD","executionInfo":{"status":"ok","timestamp":1624215322568,"user_tz":-330,"elapsed":33,"user":{"displayName":"Akash Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitYhMLI_o2TmDi-sZDOgdYyI8RJFvVdIV0-pHwIg=s64","userId":"08911264043000151771"}},"outputId":"0b882319-19aa-4d41-d95d-a0d76d8da18b"},"source":["data.head(25)"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sample code number</th>\n","      <th>Clump Thickness</th>\n","      <th>Uniformity of Cell Size</th>\n","      <th>Uniformity of Cell Shape</th>\n","      <th>Marginal Adhesion</th>\n","      <th>Single Epithelial Cell Size</th>\n","      <th>Bare Nuclei</th>\n","      <th>Bland Chromatin</th>\n","      <th>Normal Nucleoli</th>\n","      <th>Mitoses</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1000025</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1002945</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>10</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1015425</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1016277</td>\n","      <td>6</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1017023</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1017122</td>\n","      <td>8</td>\n","      <td>10</td>\n","      <td>10</td>\n","      <td>8</td>\n","      <td>7</td>\n","      <td>10</td>\n","      <td>9</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1018099</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>10</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1018561</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1033078</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1033078</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1035283</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>1036172</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>1041801</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1043999</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>1044572</td>\n","      <td>8</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>10</td>\n","      <td>7</td>\n","      <td>9</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>1047630</td>\n","      <td>7</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>1048672</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>1049815</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>1050670</td>\n","      <td>10</td>\n","      <td>7</td>\n","      <td>7</td>\n","      <td>6</td>\n","      <td>4</td>\n","      <td>10</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>1050718</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>1054590</td>\n","      <td>7</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>10</td>\n","      <td>5</td>\n","      <td>10</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>1054593</td>\n","      <td>10</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>6</td>\n","      <td>7</td>\n","      <td>7</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>1056784</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>1057013</td>\n","      <td>8</td>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>?</td>\n","      <td>7</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>1059552</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Sample code number  Clump Thickness  ...  Mitoses  Class\n","0              1000025                5  ...        1      2\n","1              1002945                5  ...        1      2\n","2              1015425                3  ...        1      2\n","3              1016277                6  ...        1      2\n","4              1017023                4  ...        1      2\n","5              1017122                8  ...        1      4\n","6              1018099                1  ...        1      2\n","7              1018561                2  ...        1      2\n","8              1033078                2  ...        5      2\n","9              1033078                4  ...        1      2\n","10             1035283                1  ...        1      2\n","11             1036172                2  ...        1      2\n","12             1041801                5  ...        1      4\n","13             1043999                1  ...        1      2\n","14             1044572                8  ...        4      4\n","15             1047630                7  ...        1      4\n","16             1048672                4  ...        1      2\n","17             1049815                4  ...        1      2\n","18             1050670               10  ...        2      4\n","19             1050718                6  ...        1      2\n","20             1054590                7  ...        4      4\n","21             1054593               10  ...        1      4\n","22             1056784                3  ...        1      2\n","23             1057013                8  ...        1      4\n","24             1059552                1  ...        1      2\n","\n","[25 rows x 11 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"wSVFZHEsE7ir"},"source":["> Now we can see for the index 23 there is a value '?'. It means there should be other values like this for which it become object type. Our next task will be to replace '?' with NaN as the value is undefined."]},{"cell_type":"code","metadata":{"id":"hc8HZAEVA0wJ","executionInfo":{"status":"ok","timestamp":1624215322570,"user_tz":-330,"elapsed":31,"user":{"displayName":"Akash Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitYhMLI_o2TmDi-sZDOgdYyI8RJFvVdIV0-pHwIg=s64","userId":"08911264043000151771"}}},"source":["data[\"Bare Nuclei\"].replace('?',np.nan, inplace=True)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9xERmXVEMGfQ"},"source":["> Again we will check the dataset information now."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TNzSLWWsMNcj","executionInfo":{"status":"ok","timestamp":1624215322575,"user_tz":-330,"elapsed":35,"user":{"displayName":"Akash Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitYhMLI_o2TmDi-sZDOgdYyI8RJFvVdIV0-pHwIg=s64","userId":"08911264043000151771"}},"outputId":"be3a774a-937b-42ef-d314-08392253bf32"},"source":["data.info()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 699 entries, 0 to 698\n","Data columns (total 11 columns):\n"," #   Column                       Non-Null Count  Dtype \n","---  ------                       --------------  ----- \n"," 0   Sample code number           699 non-null    int64 \n"," 1   Clump Thickness              699 non-null    int64 \n"," 2   Uniformity of Cell Size      699 non-null    int64 \n"," 3   Uniformity of Cell Shape     699 non-null    int64 \n"," 4   Marginal Adhesion            699 non-null    int64 \n"," 5   Single Epithelial Cell Size  699 non-null    int64 \n"," 6   Bare Nuclei                  683 non-null    object\n"," 7   Bland Chromatin              699 non-null    int64 \n"," 8   Normal Nucleoli              699 non-null    int64 \n"," 9   Mitoses                      699 non-null    int64 \n"," 10  Class                        699 non-null    int64 \n","dtypes: int64(10), object(1)\n","memory usage: 60.2+ KB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t3s2RMMBMTmM"},"source":["> Still we can see the datatype for 'Bare Nuclei' is object. So we need to convert it to numeric values."]},{"cell_type":"code","metadata":{"id":"n42jUjdMG-f1","executionInfo":{"status":"ok","timestamp":1624215322576,"user_tz":-330,"elapsed":30,"user":{"displayName":"Akash Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitYhMLI_o2TmDi-sZDOgdYyI8RJFvVdIV0-pHwIg=s64","userId":"08911264043000151771"}}},"source":["data[\"Bare Nuclei\"] = pd.to_numeric(data[\"Bare Nuclei\"])"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B2W_YnQhNCfF"},"source":["> Now checking the datatype again."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ZUZKQ5LNGKA","executionInfo":{"status":"ok","timestamp":1624215322577,"user_tz":-330,"elapsed":30,"user":{"displayName":"Akash Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitYhMLI_o2TmDi-sZDOgdYyI8RJFvVdIV0-pHwIg=s64","userId":"08911264043000151771"}},"outputId":"9721900c-81f5-47e6-f47b-8ac4b3129bad"},"source":["data.info()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 699 entries, 0 to 698\n","Data columns (total 11 columns):\n"," #   Column                       Non-Null Count  Dtype  \n","---  ------                       --------------  -----  \n"," 0   Sample code number           699 non-null    int64  \n"," 1   Clump Thickness              699 non-null    int64  \n"," 2   Uniformity of Cell Size      699 non-null    int64  \n"," 3   Uniformity of Cell Shape     699 non-null    int64  \n"," 4   Marginal Adhesion            699 non-null    int64  \n"," 5   Single Epithelial Cell Size  699 non-null    int64  \n"," 6   Bare Nuclei                  683 non-null    float64\n"," 7   Bland Chromatin              699 non-null    int64  \n"," 8   Normal Nucleoli              699 non-null    int64  \n"," 9   Mitoses                      699 non-null    int64  \n"," 10  Class                        699 non-null    int64  \n","dtypes: float64(1), int64(10)\n","memory usage: 60.2 KB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1kpCs3V3NJio"},"source":["> We can see it has been succesfully converted to proper numeric format. Our next task is to replace all the NaN values with some other values like mean of the column or anything else like that. But we will do that after we split the dataset into train set and test set. Because if we take the mean before splitting the dataset then there will be information leakage from the test set to the train set which is totally undesirable."]},{"cell_type":"code","metadata":{"id":"sPv1YEXVOl9a","executionInfo":{"status":"ok","timestamp":1624215322579,"user_tz":-330,"elapsed":28,"user":{"displayName":"Akash Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitYhMLI_o2TmDi-sZDOgdYyI8RJFvVdIV0-pHwIg=s64","userId":"08911264043000151771"}}},"source":["X = data.iloc[:, 1:-1].values\n","y = data.iloc[:, -1].values"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"go004i5OOtPE"},"source":["> We have converted our dataset into independent variable (X) and our dependent variable (y). You can see that we have removed the first column 'Sample code number' as it is just a random number and it will not be useful to create any impact on classification. Now X and y can be used to create the train set and test set."]},{"cell_type":"markdown","metadata":{"id":"mOzslMkdPWnO"},"source":["## Dataset Splitting into Train and Test Set"]},{"cell_type":"code","metadata":{"id":"zXr7shoOPyV8","executionInfo":{"status":"ok","timestamp":1624215323020,"user_tz":-330,"elapsed":468,"user":{"displayName":"Akash Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitYhMLI_o2TmDi-sZDOgdYyI8RJFvVdIV0-pHwIg=s64","userId":"08911264043000151771"}}},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZrQTwYUEP2eX"},"source":["> To create the train and test set we have used the Scikit Learn library and train_test_split class is actually doing the task for us. Here testing set size is 20%, so the training set size will be 80%."]},{"cell_type":"markdown","metadata":{"id":"if5ig4qmiShe"},"source":["## Preprocesing the Training Dataset (X_train)"]},{"cell_type":"markdown","metadata":{"id":"T5DA8vnekK3E"},"source":["> Now we will replace the NaN values in the training data using the mean of that column."]},{"cell_type":"code","metadata":{"id":"CwQRPdz_gBhi","executionInfo":{"status":"ok","timestamp":1624215323021,"user_tz":-330,"elapsed":17,"user":{"displayName":"Akash Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitYhMLI_o2TmDi-sZDOgdYyI8RJFvVdIV0-pHwIg=s64","userId":"08911264043000151771"}}},"source":["X_train[np.isnan(X_train[:,5]),5] = np.nanmean(X_train[:,5])"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CxKz4HC3kuh_"},"source":["> Here nanmean will calculate mean values of column at index 5 which is 'Bare Nuclei'. But it will not take the NaN values to calculate the average as it will again produce NaN. \n","\n","> After that it will check all the row index which has NaN values at column index 5 and then it will replace all those NaN values at column index 5 with the calculated mean."]},{"cell_type":"markdown","metadata":{"id":"eCweA5OFmvPW"},"source":["## Preprocesing the Testing Dataset (X_test)"]},{"cell_type":"markdown","metadata":{"id":"UuT49qc4nAFU"},"source":["> Preprocessing the testing data is necessary. Because if it has NaN values in any of it's rows, then our regression model will unable to find the correct class of it. So we will remove all the rows in the test set which has the NaN values. We will not use the mean here to replace the NaN values as we can not use other values in testing data set to predict the output."]},{"cell_type":"code","metadata":{"id":"DQA1HbYlsS9a","executionInfo":{"status":"ok","timestamp":1624215323022,"user_tz":-330,"elapsed":17,"user":{"displayName":"Akash Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitYhMLI_o2TmDi-sZDOgdYyI8RJFvVdIV0-pHwIg=s64","userId":"08911264043000151771"}}},"source":["y_test = y_test[~np.isnan(X_test[:,5])]"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZHa4joc-tbJX"},"source":["> At first we have deleted all the rows of y_test using X_test. Now we will delete rows from X_test."]},{"cell_type":"code","metadata":{"id":"G6-pKs5Cok67","executionInfo":{"status":"ok","timestamp":1624215323023,"user_tz":-330,"elapsed":18,"user":{"displayName":"Akash Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitYhMLI_o2TmDi-sZDOgdYyI8RJFvVdIV0-pHwIg=s64","userId":"08911264043000151771"}}},"source":["X_test = X_test[~np.isnan(X_test[:,5]),:]"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BemstUHzqApq"},"source":["> Here we have not taken any rows of column index 5 which has NaN values.\n","\n","> Now our training and testing set is ready. So we can proceed to the model training part."]},{"cell_type":"markdown","metadata":{"id":"CohHCcrmqpo1"},"source":["## Logistic Regression Model Training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fDiEFLFZrCPg","executionInfo":{"status":"ok","timestamp":1624215323023,"user_tz":-330,"elapsed":17,"user":{"displayName":"Akash Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitYhMLI_o2TmDi-sZDOgdYyI8RJFvVdIV0-pHwIg=s64","userId":"08911264043000151771"}},"outputId":"ee315102-afc2-41a6-9ee2-cc459f10093c"},"source":["from sklearn.linear_model import LogisticRegression\n","classifier = LogisticRegression()\n","classifier.fit(X_train, y_train)"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n","                   multi_class='auto', n_jobs=None, penalty='l2',\n","                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"oPlMWhdHtyv4"},"source":["> To train the model we used LogisticRegression class from Skicit Learn library. Then we fit out model using X_train and y_train. Our model is ready now to predict the output of testing set."]},{"cell_type":"markdown","metadata":{"id":"YZHgvCAIuYff"},"source":["## Prediction of Breast Cancer using the Testing Data"]},{"cell_type":"code","metadata":{"id":"3n54vqwErMV8","executionInfo":{"status":"ok","timestamp":1624215323024,"user_tz":-330,"elapsed":15,"user":{"displayName":"Akash Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitYhMLI_o2TmDi-sZDOgdYyI8RJFvVdIV0-pHwIg=s64","userId":"08911264043000151771"}}},"source":["y_pred = classifier.predict(X_test)"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vj7Hh29hurYI"},"source":["> Here we stored the prediction output into y_pred array. Now we will create the confusion matrix to check the accuracy of our model."]},{"cell_type":"markdown","metadata":{"id":"N6ZbIvygvNQ9"},"source":["## Confusion Matrix"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U5ek7foyrPU9","executionInfo":{"status":"ok","timestamp":1624215323025,"user_tz":-330,"elapsed":16,"user":{"displayName":"Akash Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitYhMLI_o2TmDi-sZDOgdYyI8RJFvVdIV0-pHwIg=s64","userId":"08911264043000151771"}},"outputId":"cfbd9dca-9291-4f03-eb6e-08b5fc7126c5"},"source":["from sklearn.metrics import confusion_matrix, accuracy_score\n","cm = confusion_matrix(y_test, y_pred)\n","print(cm)\n","accuracy_score(y_test, y_pred)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["[[88  2]\n"," [ 2 44]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.9705882352941176"]},"metadata":{"tags":[]},"execution_count":16}]}]}